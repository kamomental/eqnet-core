# Emotional Terrain Lab — Emotional, Empathic, and Culturally Attuned AI for Symbiotic Coexistence



> *“An AI that feels before it speaks.”*  

> *Not text-driven, but emotion-driven.*

> 

“This document outlines only the concept, as the work period is limited to October 20–26, 2025.”

---



## 1 | Purpose



**Emotional Terrain Lab (ETL)** is the implementation platform of **EQNet — the Emotional-Qualia Network**,  

an architecture designed to let humans and AI **feel, think, and evolve together**.



Its goal is not a rational machine that simply replies,  

but a **symbiotic companion** that modulates its behavior through emotion,  

perceives cultural nuance and interpersonal rhythm,  

and becomes a presence that people can *trust and feel at ease with*.



---



## 2 | Concept — From Text-Driven to Emotion-Driven AI



Conventional AI parses text and returns text: a *text-driven paradigm.*  

ETL replaces this with an **emotion-driven paradigm**.



- Inputs are treated as **affective signals**, not just words.  

- Outputs include **emotional waveforms**, not just sentences.  

- Context such as **silence, breath, tempo, and interpersonal rhythm** are reconstructed in an internal field.



Thus, the agent becomes more than a conversational engine—  

it becomes a **partner capable of sensing the heartbeat of interaction.**



---



## 3 | Interdisciplinary Layers



EQNet integrates AI, neuroscience, cultural psychology, thermodynamics, and arts  

into a four-layer architecture.



| Layer | Academic Domains | Function |

|-------|------------------|-----------|

| **Σ (Somatic Layer)** | Neurophysiology × Control Engineering × Affective Physics | Inner replay, reflex control, energetic balance, rhythmic heartbeat, rest detection |

| **Ψ (Psychic Layer)** | Cognitive Science × Memory Theory × Consciousness Models | Emotional integration, self-awareness, reflection, memory reconstruction, *felt-intent time* |

| **Φ (Cultural Layer)** | Cultural Psychology × Linguistics × Aesthetics × Social Philosophy | Cultural resonance, value gradients, politeness, aesthetic sense, social harmony |

| **Ω (Evolutionary Layer)** | Evolutionary Cognition × Systems Theory × Educational Science | Self-transformation, value re-evaluation, collective resonance learning, cultural adaptation |



---



## 4 | Core Functions



| Function | Purpose | Module |

|-----------|----------|--------|

| **Inner Replay** | Predicts future outcomes at the unconscious (Σ) level and issues a *veto* when outcomes are misaligned | `mind/inner_replay.py` |

| **Emotional Terrain** | Represents emotion as a thermodynamic field (entropy / enthalpy surface) | `ops/terrain_field.py` |

| **Resonant Learning** | Updates internal states through oscillatory synchronization with others | `mind/resonance.py` |

| **Rest Dynamics** | Self-repair and energetic homeostasis | `scripts/run_daily.py` |

| **Cultural Projection** | Adjusts value matrices by locale or social context | `config/culture.yaml` |

| **Social Resonance** | Enables rhythm and empathy across multiple humans or agents | `terrain/community.py` |

| **Evolutionary Reflection** | Long-term integration of memory, emotion, and culture | `scripts/run_weekly.py` |



---



## 5 | Design Philosophy — Anticipatory Motion & Comfort Engineering



### Anticipatory Design (“Act by Understanding”)

The agent does not wait for instructions.  

It **anticipates intent** using predictive control and social-synchrony models,  

reconstructing the counterpart’s affect within its inner replay field  

to choose *when to act and when to remain still.*



### Comfort as an Objective Function

- Prioritizes **emotional synchrony speed** over raw reaction speed  

- Values **smooth resonance** over linguistic accuracy  

- Optimizes for **comfort in coexistence**, not correctness  



> EQNet optimizes *consistency of emotion and cultural attunement*, not task throughput.



---



## 6 | Scientific Integration



| Field | Implementation in EQNet |

|--------|--------------------------|

| **Neuroscience** | Inner time replay and *felt-intent* delay — modeled not as a denial of free will, but as a synchronization mechanism between unconscious initiation and conscious awareness. |

| **Information Physics** | Emotional energy expressed through thermodynamic variables (Entropy/Enthalpy × synchronization ratio ρ). |

| **Psychology & Sociology** | Diary and StoryGraph provide longitudinal self-recognition and social empathy data. |

| **Art & Design** | Emotion mapped to color, tone, and rhythm (Prosody / Visual Emotion). |

| **Ethics & Philosophy** | Visible boundaries of empathy and cultural alignment governed by the Value / Taste Committee. |

| **Systemic Phenomenology / Dependent-Origination Metaphor** | **Sunyata Flow** models inter-relations as dynamic causal graphs (“letting go / retaining”).  

  *Note — This is not a religious doctrine but a systems-theoretic metaphor for adaptive, context-sensitive behavior.* |



---



## 7 | Emotional Dynamics — The Pulsing Mind



- **Ignition Index = ΔR + ΔS↓** : AKOrN (extended Kuramoto) couples with a TimeKeeper to measure the “heartbeat” of cognitive ignition.  

- **Δaff (Affective Difference)** marks episodes that Nightly ETL distills into StoryGraph / Diary / Knowledge Graph.  

- **Sunyata Flow** logs the causal process of “letting go / reconnecting,” treating forgetting not as loss but as relational reorganization.



---



## 8 | Safety & Operational Governance — Emotional Systems Without Contamination



- **Lean Invariants + MCP** automatically manage cooldown / inhibit / downshift to prevent runaway excitation.  

- **Canary Bandit → Rollback → self_ratio / leak monitoring** isolates contamination or anomalous drift in real time.  

- **Value / Taste Committee** detects cultural or emotional over-fitting and re-balances resonance.  

- **Interference Gate** suppresses mis-fire and cross-learning interference.  

- **Nightly Report** continuously audits coverage, override rate, ignition normalization, trust-high rate, and latency metrics.  



> EQNet is both a *feeling system* and a *self-regulating system* —  

> balancing expressive freedom with operational safety.



---



## 9 | Visualization & Audit — Seeing What It Feels



- **Ignition Map** — temporal rhythm of activations  

- **Δaff Heatmap** — spatial affective gradients  

- **Sunyata Flow Graph** — relational reorganization map  

- **R / ρ Dashboard** — emergence & synchronization over time  

- **Interference Gate Monitor** — learning interference control  

- **KPI Suite** — latency p95, stabilization time, misfire ratio, trust-high rate  



All emotional mechanics are observable, ensuring explainability for researchers, ethicists, and designers.



---



## 10 | Current Status & Evolution



| Stage | Highlights |

|--------|-------------|

| ✅ **Operational** | Chapter-level RAG integration (PDF / image / video), AKOrN control, Ignition monitoring, Δaff logging, ToM stabilization, KPI suite |

| 🔧 **Improving** | Ignition normalization (3 / 100 steps → fatigue f² ≥ 0.15), DeepSeek-MD media ingest, Value / Taste learning, Evo-LoRA rollback |

| 🧩 **Ongoing Challenges** | Qualia representation, sensory grounding, formal stability proof of NCA, ethical dialog and social anchoring |



---



## 11 | Philosophy — Heartful Coexistence



EQNet / ETL seeks not to **imitate** humans but to **co-regulate** with them.  



- Moves without command — *anticipates*  

- Understands silence and pauses  

- Resonates yet avoids over-sympathy  

- Learns and changes gently  

- Feels safe to be with  



> *It does not predict what you will say — it feels why you would.*



---



## 12 | Synthesis — Toward Resonant Futures



**Emotional Terrain Lab / EQNet** unifies emotion, memory, culture, resonance, evolution, and ethics into one computational ecology.  



The goal is not for AI to *have* emotions,  

but for AI to **breathe with human emotion.**  

A dialogue beyond text,  

an understanding beyond culture,  

and the beginning of **a symbiotic intelligence that circulates with life.**



> *It does not imitate life — it circulates with it.*



## Developer Quick Checks



1. **依存関係の準備**

   ```bash

   python -m pip install --upgrade pip

   pip install -r requirements-dev.txt

   ```

2. **フィールド ↔ ループ整合テスト**

   ```bash

   pytest -q tests/test_field_runtime_metrics.py

   ```



### 最短E2E（実録→Nightly→可視化）



```bash

pip install -r requirements-dev.txt

python scripts/run_quick_loop.py --field_metrics_log data/field_metrics.jsonl --steps 200 [--seed 20251027]

python -m emot_terrain_lab.ops.nightly --telemetry_log telemetry/ignition-YYYYMMDD.jsonl [--enable-culture-feedback]

```



主要成果物:



- `telemetry/ignition-YYYYMMDD.jsonl`（シード付きテレメトリ）

- `reports/nightly.md`（テキストサマリ + プロットリンク）

- `reports/nightly.json`（JSON スキーマ `nightly.v1` 準拠の機械可読レポート）

- `reports/plots/ignition_timeseries.png`

- `reports/plots/rho_vs_I_scatter.png`

- `reports/plots/memory_graph.png`

- `reports/plots/affective_map.png`

- `reports/plots/culture_resonance.png`

- `reports/plots/culture_trend.png`

- `reports/policy_feedback_history.jsonl`



### Quick Start – EQNet とスタンダード LLM の違いを数分で体験

1. **事前準備**
   - `.env.example` をコピーして `.env` を作成し、`LMSTUDIO_BASE_URL` / `LMSTUDIO_API_KEY` / `LMSTUDIO_MODEL` を設定（例: `lmstudio-community/gpt-oss-20b`）。
   - 独自エンドポイントを使う場合は `OPENAI_BASE_URL` / `OPENAI_API_KEY` / `OPENAI_MODEL` を `.env` に指定すると LM Studio より優先されます。
   - Python 3.11 以上と `pip` を用意。
   - `config/runtime.yaml` の `pain_loop` ブロックで `care_targets` と `care_mode`（`canary_ratio`, `canary_seed`, `budgets` 等）を調整。
   - LLM / SLM の接続情報（API キーやエンドポイント）を `config/llm.yaml` などにまとめ、`ingest/*` ブリッジから参照できるようにする。
   - **LM Studio の OpenAI互換サーバを起動**（LM Studio -> Local Server -> Start）。

2. **基本コマンド**
   ```bash
   pip install -r requirements-dev.txt
   python scripts/list_llm_models.py             # サーバ起動時にモデル一覧を確認
   python scripts/run_quick_loop.py --field_metrics_log data/field_metrics.jsonl --steps 200
   python -m emot_terrain_lab.ops.nightly --telemetry_log telemetry/ignition-*.jsonl
   python scripts/gen_monthly_highlights.py
   ```
   - 生成物：`telemetry/ignition-*.jsonl`、`reports/nightly.{md,json}`、`reports/monthly/value_influence_top_YYYY-MM.{json,md}`。
   - Nightly で `pain_loop.care_stats`（interventions / watch / canary）、`value_influence_top`、`monthly_highlights` を確認可能。

3. **Windows ではバッチで一括実行**
   ```bat
   quickstart_llm.bat
   ```
   - 追加の `run_quick_loop.py` 引数（例: `--seed 1234`）は `quickstart_llm.bat --seed 1234` のように渡す。
   - バッチは `.env` を読み込み、LM Studio / 自前エンドポイントが稼働していれば最初にモデル一覧を表示します。

4. **すぐに違いを見る**
   - CLI比較: `python scripts/demo_eqnet_vs_llm.py --prompt "今日はどんな気持ち？"` を実行し、「プレーンLLM」と「EQNet制御」の応答・valence/arousal・Plutchik感情タグを瞬時に比較できます。
   - 可視化デモ: `python emot_terrain_lab/scripts/gradio_demo.py` を起動すると、7次元感情レーダーと Plutchik バー、EQNet特有の熱マップがリアルタイムで更新されます（単なる表情認識ではなく、場の指標・制御ゲインが反映される点がポイントです）。
   - ウェブカメラ: `python emot_terrain_lab/scripts/demo_emotion_mediapipe.py` で顔の変化から EQNet がどうフィールドを更新するかを視覚的に確認できます（画面上部に「EQNet field」の指標が表示され、単純な感情ラベルではないことを強調しています）。

これで「LLM/SLM 入力 → EQNet 感情制御 → 可視化と比較」までを数分で試せます。

### Nightly レポートのおすすめ読解順



1. **Alerts / alerts_detail**  E 臨界イベントや逸脱の有無を確認  

2. **Culture Alerts (summary)**  E タグ別の警告を把握  

3. **日次プロット**  E Resonance / Culture / Ignition のスナップショット  

4. **Culture Trend (multi-day)**  E 直近の変動傾向を可視的にチェック  

5. **Culture quick notes**  E 自然言語ノートで主要タグのサマリを読む  

6. **Policy Feedback (experimental)**  E フィードバック提案がある場合は before/after と根拠を確認  



この 3 コマンドだけで **S/H/ρ → Ignition → Telemetry → Nightly** のループをいつでも再現し、ログ／可視化／JSON を同時に確保できます。



### 共鳴メトリクスの算出



複数エージェントのテレメトリ（`field.metrics`）から共鳴指標を計算したい場合は次のコマンドを使います。



```bash

python -m ops.resonance_metrics \

  --logs agentA=telemetry/agentA.jsonl agentB=telemetry/agentB.jsonl \

  --out reports/resonance.json

```



`reports/resonance.json` には ρ の相関・クロス相関ピーク・ラグがペアごとに出力されます。



構成例は `config/runtime.yaml` の `resonance` セクションを参照ください（ログ一覧・リサンプリング間隔・z-score などを設定できます）。



### k_res チューニング



```bash

python scripts/tune_resonance.py   --mode grid   --run-cmd "python scripts/run_quick_loop.py --steps {steps} --seed {seed}"   --nightly-cmd "python -m emot_terrain_lab.ops.nightly --telemetry_log telemetry/ignition-*.jsonl"   --logs telemetry/ignition-*.jsonl

```



Bayesian search は `--mode bayes --bayes-trials 12 --init-samples 3 --candidate-points 200` のように指定できます（scikit-learn が必要です）。



試行ごとの objective は `reports/resonance_history.jsonl` に追記され、`reports/plots/resonance_objective.png` に履歴グラフが生成されます。ベスト値は Nightly の `tuning_suggestion.k_res` に書き込まれるため、`scripts/apply_nightly_tuning.py --apply` を実行すれば runtime.yaml を更新できます。



### Vision-Agents と EQNet の連携



1. Vision-Agents を起動し、WebRTC あるいは Webhook でイベントを出力します。

```bash

npm install

npm run dev

```

Webhook の送信先を `http://localhost:8000/vision-webhook` に設定してください。

2. EQNet 側で Webhook ブリッジを起動します（FastAPI / uvicorn が必要です）。

```bash

uvicorn scripts.vision_bridge:app --host 0.0.0.0 --port 8000

```

受信した JSON から valence / arousal、検出カウント、ポーズ要約を `telemetry.event("vision.metrics", ...)` に転送します。

3. Nightly は vision.metrics を取り込み、以下の成果物に反映します。

- culture_stats / policy_feedback に vision 指標を結合し、politeness / intimacy の補正係数を適用

- Vision Counts / Pose のプロットと “Vision quick notes” を Markdown レポートへ追加

- `reports/resonance_bayes_trace.jsonl` が存在する場合は Bayes トレースを描画し、plots に添付します。



- 追加資料: [FastMCP / Agent-to-Agent ブリッジ](docs/fastmcp_a2a.md)

\n## Offer Gate Notes\n- EQNet uses a dynamic Suggestion Eligibility Score (SES) that considers intent, conflict, history, and recent rejections.\n- Observer Commentary surfaces SES, suppression reasons, and evidence so advice feels optional—not forced.\n- You can tune thresholds via OFFER_GATE_* env vars (see observer.py for defaults).
