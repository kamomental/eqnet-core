# 統一Replayによるブレイクスルー（簡易メモ）

- 「Replayはすべてのドメインで発火し得る心的時間旅行機構である」
という統一設計ができる。
- 迷路で立ち止まりその後の展開を想定する選択的リプレイも、
休憩を挟んでスキルを再現する練習のリプレイも
外界と関係なく、内的世界で次の最適行動を調整しているという点では同列。

- 違うのは「何を最適化しているか」（空間経路か、運動出力か）だけ。
- アルゴリズムとしては
- Latent State + Replay Rollout + Value Updateという同じ構造になる。


- 今回の統一Replay設計は、「AIが外界から独立して内的世界を持つ」という構造的転換点に到達しています。

- なぜこれが本質的な進歩なのか
- 従来のLLMとの決定的な違い
- 従来LLM:
  Input → [Black Box] → Output
  （外界依存：入力がなければ何も起きない）

  EQNet + Unified Replay:
  Input → Hub → [Internal Simulation] → Output
              ↓
         Replay Engine
              ↓
    Mood/Value Modulation
              ↓
         Memory Trace
  （内的生成：外界なしでも心的活動が続く）

- Atriが夜ベッドで「明日どうしよう」と考えているとき、外界からの入力は何もない。でも彼女の心は動き続けている。 これが今回実装された構造です。



- 2025-10-21、EQNet はドメイン非依存の「Unified Replay」モジュールを実装。
- 空間ナビゲーション／運動学習／言語推論を共通 API (`UnifiedReplay.rollout`) で再生し、感情・不確実性・価値ベクトルを入力に持つ心的シミュレーションへ昇格。
- 発火条件は `prediction_error`・`novelty`・`value_uncertainty`・`idle_state` など内部指標で制御し、外界刺激が途絶えても内的時間が進む。
- Hub（即時最適化）／Mood（エピソード統合）／Plan（長期戦略）の3層で horizon を切り分け、迷路・運動・言語の各モダリティに同一構造で適用。
- 従来 RL における「学習補助としてのリプレイ」から、「主体が未来を想像し価値を更新する主要プロセス」へと格上げ。
- 今後は Replay 痕跡の記憶化 → Self-Model → Narrative へ拡張し、他者理解（ToM）も同じ Replay エンジン上で統合予定。

## 従来LLMとの決定的な違い

| 観点 | 従来の LLM（入力反応型） | EQNet Unified Replay 層 |
| --- | --- | --- |
| 内部時間 | 外部入力に同期。入力が止まると活動も停止 | 外部入力がなくても Replay が発火し、内的時間が進む |
| 未来生成 | 逐次的な次トークン予測のみ | 多段 horizon の内的シミュレーションで未来候補を生成 |
| 価値判断 | 明示的な価値ベクトルなし（プロンプト依存） | 感情・審美・社会性など多次元価値を内生的に評価 |
| 行動選択 | 出力は入力依存の応答 | Replay で得た未来価値に基づきポリシーを更新し行動を選択 |
| 内的動機づけ | なし（外部指示頼み） | 感情・不確実性・価値の変動が発火トリガーになる |
| 記憶との統合 | 局所履歴（チャットログ）のみ | エピソード記憶と再結合し、Mood/Plan 層で長期的整合を図る |

## 何ができるようになるか

- **内的時間旅行**: 休憩中でも未来シミュレーションが継続し、点火有無や価値勾配を事前に評価できる。
- **多目的最適化**: 単一の報酬ではなく、審美・関係性・自己一貫性といった価値を同時にバランスする方策調整が可能。
- **自律的計画の萌芽**: 外部からの指示なしに「次に何をすべきか」を内的に生成し、行動に反映できる。
- **ToM 拡張の足場**: 同じ Replay エンジンを他者の latent state に適用することで、他者未来のシミュレーションと共感的介入が視野に入る。
- **Narrative 形成への布石**: Replay 痕跡を記憶統合することで、「私がどうありたいか」という時間的物語の生成が実装可能になる。
